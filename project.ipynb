{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winsonchow/Documents/GitHub/RAG-app/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/winsonchow/Documents/GitHub/RAG-app/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# System imports\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python libraries\n",
    "import requests\n",
    "\n",
    "# Gradio imports\n",
    "import gradio as gr\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# BeautifulSoup imports\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Spacy imports\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from the environment variable\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API endpoints\n",
    "list_posts_url = 'https://www.techinasia.com/wp-json/techinasia/2.0/posts'\n",
    "search_posts_url = 'https://www.techinasia.com/wp-json/techinasia/2.0/articles?query='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Posts from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch posts from API\n",
    "def fetch_posts_from_api(page):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(list_posts_url, params={'page': page}, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        # print(\"Response Headers:\", response.headers)\n",
    "        # print(\"Response Text:\", response.text)\n",
    "\n",
    "        post_data = response.json()\n",
    "\n",
    "        return post_data['posts']\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error fetching posts: {e}')\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f'Error parsing response: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch posts\n",
    "page = 1\n",
    "posts = fetch_posts_from_api(page=page)\n",
    "print(f\"Fetched {len(posts)} posts.\")\n",
    "\n",
    "# # Display the first post to verify the structure\n",
    "# if posts:\n",
    "#     print(posts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Posts from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search posts from API\n",
    "def search_posts(query):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(search_posts_url, params={'query': query}, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        # print(\"Response Headers:\", response.headers)\n",
    "        # print(\"Response Text:\", response.text)\n",
    "\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error fetching posts: {e}')\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f'Error parsing response: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    doc = nlp(query)\n",
    "    keywords = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            # Add named entities and nouns to keywords list\n",
    "            if token.ent_type_ or token.pos_ in ['NOUN', 'PROPN', 'ADJ']:\n",
    "                keywords.append(token.lemma_)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['Grab', 'profitability', 'recent', 'news']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "user_query = \"Tell me about Grabâ€™s profitability and recent news.\"\n",
    "keywords = preprocess_query(user_query)\n",
    "print(\"Extracted Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for posts with a keyword\n",
    "query = \"Apple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch search results\n",
    "search_results = search_posts(query)\n",
    "\n",
    "# Display the search results\n",
    "if search_results and 'posts' in search_results and 'hits' in search_results['posts']:\n",
    "    search_posts = search_results['posts']['hits']\n",
    "\n",
    "    # Articles with title and content\n",
    "    raw_articles = \"\".join([post['title'] + post['content'] for post in search_posts[:3]])\n",
    "else:\n",
    "    raw_articles = 'No relevant posts found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving The Most Relevant Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Handle special characters\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Normalise whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove source and citation tags\n",
    "    text = re.sub(r'\\[.*?\\]+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "cleaned_articles = preprocess_text(raw_articles)\n",
    "print(cleaned_articles)\n",
    "\n",
    "# Length of cleaned content\n",
    "print(f\"Length of raw content: {len(raw_articles)}\")\n",
    "print(f\"Length of cleaned content: {len(cleaned_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process search results and generate background knowledge\n",
    "def generate_context(search_results):\n",
    "    if search_results and 'posts' in search_results and 'hits' in search_results['posts']:\n",
    "        search_posts = search_results['posts']['hits']\n",
    "\n",
    "        # background knowledge with title and content\n",
    "        raw_articles = \"\".join([post['title'] + post['content'] for post in search_posts[:3]])\n",
    "    else:\n",
    "        raw_articles = 'No relevant posts found.'\n",
    "    return raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses using OpenAI\n",
    "def generate_response(search_results, query):\n",
    "    try:\n",
    "        background_knowledge = generate_context(search_results)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer the question based on the question asked and background knowledge provided below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {query}\\nBackground Knowledge: {background_knowledge}\\nAnswer:\"}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error generating response: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the response generation\n",
    "# query = 'How is Grab performing in the market?'\n",
    "\n",
    "# search_results = search_posts(query)\n",
    "# generate_response(search_results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Gradio interface\n",
    "def rag_interface(query):\n",
    "    search_results = search_posts(query)\n",
    "    return generate_response(search_results, query)\n",
    "\n",
    "iface = gr.Interface(fn=rag_interface, inputs=\"text\", outputs=\"text\", title='Tech in Asia RAG System', description='Ask a question about Tech in Asia and get an answer based on the context of the latest posts.')\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
