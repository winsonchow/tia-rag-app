{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winsonchow/Documents/GitHub/RAG-app/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/winsonchow/Documents/GitHub/RAG-app/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# System imports\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Python libraries\n",
    "import requests\n",
    "\n",
    "# Gradio imports\n",
    "import gradio as gr\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# BeautifulSoup imports\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Spacy imports\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from the environment variable\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API endpoints\n",
    "list_posts_url = 'https://www.techinasia.com/wp-json/techinasia/2.0/posts'\n",
    "search_posts_url = 'https://www.techinasia.com/wp-json/techinasia/2.0/articles?query='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Posts from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch posts from API\n",
    "def fetch_posts_from_api(page):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(list_posts_url, params={'page': page}, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        # print(\"Response Headers:\", response.headers)\n",
    "        # print(\"Response Text:\", response.text)\n",
    "\n",
    "        post_data = response.json()\n",
    "\n",
    "        return post_data['posts']\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error fetching posts: {e}')\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f'Error parsing response: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch posts\n",
    "page = 1\n",
    "posts = fetch_posts_from_api(page=page)\n",
    "print(f\"Fetched {len(posts)} posts.\")\n",
    "\n",
    "# # Display the first post to verify the structure\n",
    "# if posts:\n",
    "#     print(posts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Posts from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search posts from API\n",
    "def search_posts(query):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(search_posts_url, params={'query': query}, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        # print(\"Response Headers:\", response.headers)\n",
    "        # print(\"Response Text:\", response.text)\n",
    "\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error fetching posts: {e}')\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f'Error parsing response: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    doc = nlp(query)\n",
    "    keywords = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            # Add named entities and nouns to keywords list\n",
    "            if token.ent_type_ or token.pos_ in ['NOUN', 'PROPN', 'ADJ']:\n",
    "                keywords.append(token.lemma_)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['Grab', 'profitability', 'recent', 'news']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "user_query = \"Tell me about Grab’s profitability and recent news.\"\n",
    "keywords = preprocess_query(user_query)\n",
    "print(\"Extracted Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "# Fetch search results\n",
    "search_results = search_posts(keywords)\n",
    "\n",
    "# Display the search results\n",
    "if search_results and 'posts' in search_results and 'hits' in search_results['posts']:\n",
    "    search_posts = search_results['posts']['hits']\n",
    "\n",
    "    # Articles with title and content\n",
    "    raw_articles = \"\".join([post['title'] + post['content'] for post in search_posts[:3]])\n",
    "else:\n",
    "    raw_articles = 'No relevant posts found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving The Most Relevant Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Handle special characters\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Normalise whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove source and citation tags\n",
    "    text = re.sub(r'\\[.*?\\]+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcast: Rounding up SEA tech news with Momentum Works Sea and Grab recently released earnings results and as always, any financial news regarding the two Southeast Asian giants gets a lot of attention. No wonder then that the first episode of the new Momentum Works podcast focused on the two firms. The venture builder and research firm recently launched Impulso, a podcast that dives into tech trends in Asia. Featuring CEO Jianggang Li, engagement manager Sabrina Chong, and insights lead Saniya Ramchandani, the episode highlights the competition Sea faces in the ecommerce space, as well as stagnating gross merchandise value for Grab. The news roundup also touches on regulations for generative AI, and Luckin Coffee’s attempted comeback. Here are four highlights we picked out from the episode: 4:45 – Why Momentum Works’ analysts estimate that Shopee is operating with a 10% take rate 16:59 – Why Grab will probably achieve its goal of profitability in the fourth quarter of this year 22:40 – Why calls to regulate artificial intelligence from OpenAI’s CEO may be self-serving 30: 50 – Why Momentum Works’ analysts aren’t sure if Luckin Coffee can make it in SingaporePaytm stock jumps 11.6% on news of Sharma’s stake purchase Vijay Shekhar Sharma, the founder and CEO of One97 Communications, has entered into an agreement with Ant Financial to buy the latter's 10.30% stake in the Indian company. The value of the 10.30% stake was estimated to be US$628 million as of August 4. The move was announced by Paytm, the payments platform operating under One97 Communications, on Monday. The purchase will be made via Sharma's owned overseas entity, Resilient Asset Management BV. With this, Sharma's ownership of Paytm will increase to 19.42%. Ant's ownership, on the other hand, will drop to 13.5%. The sale would help alleviate pressure from investors, who had been concerned over a potential share sell-off by Ant Group, and address concerns from rising India-China tensions, The Straits Times noted. The stock price of Paytm has since jumped 11.6% to 887 Indian rupees (US$10.7) apiece as of earlier today. In February, it was reported that Ant Group was mulling selling part of its stake in Paytm, which had exceeded 25% at the time. However, sources told Bloomberg that the potential sale was not driven by political reasons. \"I am proud of Paytm's role as a true champion of made-in-India financial innovation and our achievements in revolutionizing mobile payments and contributing to formal financial services inclusion in the country,\" Sharma said in a post. Founded in August 2010, Paytm started by providing prepaid mobile recharges but has since expanded into areas like ecommerce, banking, and insurance. Earlier this year, after the company narrowed its losses from US$94 million to US$47 million, Sharma said its next milestone was to generate free cash flow. See also: Paytm looks to turn things around with focus on cash flow Currency converted from Indian rupee to US dollar: US$1 = 82.7 rupees.Indonesia to require Google, Meta to compensate news publishers Indonesia has introduced a new presidential decree that will require digital platforms like Meta and Google to pay news publishers in exchange for distributing their content. The new law will go into effect six months after yesterday's signing by outgoing president Joko Widodo (Jokowi). Dubbed the Publisher Rights decree, it stipulates that digital platforms are responsible for maintaining a healthy news publishing business. Among the ways they can do so include designing a news distribution algorithm that supports quality journalism as well as partnering with news publishers. These partnerships may comprise a paid licensing arrangement, profit sharing, data sharing, or any other agreed type. In an event celebrating Indonesia's national press day, Jokowi noted that the Publisher Rights decree was initiated by media firms themselves, not the government. \"We want journalism that is high quality, journalism that is far from negative content, journalism that educate for Indonesia's development,\" he said. In a statement quoted by CNBC Indonesia, Google touted the importance of access to \"diverse news sources\" as well as an ecosystem that would help both big and small publishers to develop. Over the years, Google and Facebook have become important distribution channels for news. However, this has led to plummeting advertising revenue for publishers. Indonesia's new decree follows those already implemented in Australia and Canada, with several other countries likely to follow suit.\n",
      "Length of raw content: 6572\n",
      "Length of cleaned content: 4583\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "cleaned_articles = preprocess_text(raw_articles)\n",
    "print(cleaned_articles)\n",
    "\n",
    "# Length of cleaned content\n",
    "print(f\"Length of raw content: {len(raw_articles)}\")\n",
    "print(f\"Length of cleaned content: {len(cleaned_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process search results and generate background knowledge\n",
    "def generate_context(search_results):\n",
    "    if search_results and 'posts' in search_results and 'hits' in search_results['posts']:\n",
    "        search_posts = search_results['posts']['hits']\n",
    "\n",
    "        # background knowledge with title and content\n",
    "        raw_articles = \"\".join([post['title'] + post['content'] for post in search_posts[:3]])\n",
    "    else:\n",
    "        raw_articles = 'No relevant posts found.'\n",
    "    return raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses using OpenAI\n",
    "def generate_response(search_results, query):\n",
    "    try:\n",
    "        background_knowledge = generate_context(search_results)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer the question based on the question asked and background knowledge provided below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {query}\\nBackground Knowledge: {background_knowledge}\\nAnswer:\"}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error generating response: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the response generation\n",
    "# query = 'How is Grab performing in the market?'\n",
    "\n",
    "# search_results = search_posts(query)\n",
    "# generate_response(search_results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Gradio interface\n",
    "def rag_interface(query):\n",
    "    search_results = search_posts(query)\n",
    "    return generate_response(search_results, query)\n",
    "\n",
    "iface = gr.Interface(fn=rag_interface, inputs=\"text\", outputs=\"text\", title='Tech in Asia RAG System', description='Ask a question about Tech in Asia and get an answer based on the context of the latest posts.')\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
